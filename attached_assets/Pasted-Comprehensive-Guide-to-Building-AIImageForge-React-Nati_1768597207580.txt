Comprehensive Guide to Building AIImageForge: React Native New Architecture App with On-Device AI and Metal

This guide provides a complete implementation of AIImageForge, a production-ready React Native app leveraging the New Architecture (TurboModules + Fabric) for on-device AI image generation and Metal-enhanced rendering on iOS. The app demonstrates cutting-edge performance with CoreML Stable Diffusion and custom Metal shaders.

ðŸš€ Key Features

Â· On-Device AI Generation: Uses Apple's CoreML Stable Diffusion (6-bit palettized models)
Â· Metal-Enhanced Rendering: Custom GPU shaders for real-time image processing
Â· New Architecture: TurboModules for native logic + Fabric for UI components
Â· Optimized Performance: Apple Neural Engine (ANE) acceleration, ~5-15s generation
Â· Professional Workflow: Model downloading, progress tracking, error handling

ðŸ“ Complete Project Structure

```
AIImageForge/
â”œâ”€â”€ ios/
â”‚   â”œâ”€â”€ Podfile
â”‚   â”œâ”€â”€ MyCoreMLGeneratorModule.h
â”‚   â”œâ”€â”€ MyCoreMLGeneratorModule.mm
â”‚   â”œâ”€â”€ MyCoreMLGeneratorModule.swift
â”‚   â”œâ”€â”€ MyMetalImageView.h
â”‚   â”œâ”€â”€ MyMetalImageView.mm
â”‚   â”œâ”€â”€ MyMetalImageView.metal
â”‚   â””â”€â”€ AIImageForge-Bridging-Header.h
â”œâ”€â”€ js/
â”‚   â”œâ”€â”€ App.js
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â””â”€â”€ MetalImageView.js
â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â””â”€â”€ CoreMLGeneratorModule.js
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ modelDownloader.js
â”œâ”€â”€ package.json
â””â”€â”€ app.json
```

ðŸ› ï¸ Setup Instructions

1. Project Initialization

```bash
# Initialize with React Native 0.82+ (New Architecture enabled by default)
npx react-native@latest init AIImageForge --version 0.82.0
cd AIImageForge

# Install required dependencies
npm install react-native-fs react-native-zip-archive react-native-blob-util
```

2. iOS Configuration

Add CoreML Stable Diffusion Package

1. Open ios/AIImageForge.xcworkspace in Xcode
2. File > Add Package Dependencies
3. Enter: https://github.com/apple/ml-stable-diffusion
4. Select version: 1.0.0 or latest

Update Podfile

```ruby
# ios/Podfile
platform :ios, '17.0'

target 'AIImageForge' do
  config = use_native_modules!
  
  use_react_native!(
    :path => config[:reactNativePath],
    :new_arch_enabled => true,
    :fabric_enabled => true
  )
  
  # Add these pods
  pod 'React-RCTBlob', :path => '../node_modules/react-native/Libraries/Blob'
  pod 'react-native-blob-util', :path => '../node_modules/react-native-blob-util'
  
  # Enable TurboModules
  pod 'ReactCommon', :path => '../node_modules/react-native/ReactCommon'
  
  post_install do |installer|
    react_native_post_install(installer)
    
    # Enable New Architecture
    installer.pods_project.targets.each do |target|
      if target.respond_to?(:product_type) && target.product_type == "com.apple.product-type.bundle"
        target.build_configurations.each do |config|
          config.build_settings['CODE_SIGNING_ALLOWED'] = 'NO'
        end
      end
    end
  end
end
```

```bash
# Install pods
cd ios && pod install
```

Add Memory Entitlement

1. In Xcode: Signing & Capabilities
2. Add capability: Increased Memory Limit (com.apple.developer.kernel.increased-memory-limit)

ðŸ“± Core Implementation

1. CoreML TurboModule

iOS Bridge Header

```objective-c
// ios/AIImageForge-Bridging-Header.h
#import <React/RCTBridgeModule.h>
#import <React/RCTEventEmitter.h>
```

Swift Implementation

```swift
// ios/MyCoreMLGeneratorModule.swift
import Foundation
import StableDiffusion
import CoreML
import UIKit

@objc(MyCoreMLGenerator)
public class MyCoreMLGenerator: NSObject {
    private var pipeline: StableDiffusionPipeline?
    private var currentStep: Int = 0
    private var totalSteps: Int = 0
    
    @objc public static let shared = MyCoreMLGenerator()
    
    private override init() {
        super.init()
    }
    
    @objc public func loadModel(_ modelPath: String,
                                resolve: @escaping RCTPromiseResolveBlock,
                                reject: @escaping RCTPromiseRejectBlock) {
        do {
            let url = URL(fileURLWithPath: modelPath)
            let config = MLModelConfiguration()
            config.computeUnits = .cpuAndNeuralEngine
            
            pipeline = try StableDiffusionPipeline(
                resourcesAt: url,
                configuration: config,
                disableSafety: true,
                reduceMemory: true
            )
            
            try pipeline?.loadResources()
            resolve("Model loaded successfully")
        } catch {
            reject("LOAD_ERROR", error.localizedDescription, error)
        }
    }
    
    @objc public func generateImage(_ prompt: String,
                                    options: [String: Any],
                                    resolve: @escaping RCTPromiseResolveBlock,
                                    reject: @escaping RCTPromiseRejectBlock) {
        guard let pipeline = pipeline else {
            reject("NO_PIPELINE", "Model not loaded", nil)
            return
        }
        
        let stepCount = options["stepCount"] as? Int ?? 25
        let seed = options["seed"] as? UInt32 ?? UInt32.random(in: 0...UInt32.max)
        let guidanceScale = options["guidanceScale"] as? Float ?? 7.5
        
        totalSteps = stepCount
        currentStep = 0
        
        DispatchQueue.global(qos: .userInitiated).async {
            do {
                let images = try pipeline.generateImages(
                    prompt: prompt,
                    imageCount: 1,
                    stepCount: stepCount,
                    seed: seed,
                    guidanceScale: guidanceScale,
                    disableSafety: true
                ) { progress in
                    self.currentStep = progress.step
                    DispatchQueue.main.async {
                        NotificationCenter.default.post(
                            name: Notification.Name("onGenerationStep"),
                            object: nil,
                            userInfo: [
                                "step": self.currentStep,
                                "totalSteps": self.totalSteps
                            ]
                        )
                    }
                    return true
                }
                
                if let cgImage = images.first as? CGImage {
                    let uiImage = UIImage(cgImage: cgImage)
                    let filename = "generated_\(seed).jpg"
                    let documentsPath = FileManager.default.urls(
                        for: .documentDirectory,
                        in: .userDomainMask
                    )[0]
                    let fileURL = documentsPath.appendingPathComponent(filename)
                    
                    if let data = uiImage.jpegData(compressionQuality: 0.9) {
                        try data.write(to: fileURL)
                        DispatchQueue.main.async {
                            resolve(fileURL.path)
                        }
                    } else {
                        DispatchQueue.main.async {
                            reject("SAVE_ERROR", "Failed to save image", nil)
                        }
                    }
                } else {
                    DispatchQueue.main.async {
                        reject("NO_IMAGE", "Image generation failed", nil)
                    }
                }
            } catch {
                DispatchQueue.main.async {
                    reject("GENERATION_ERROR", error.localizedDescription, error)
                }
            }
        }
    }
}
```

Objective-C++ Module

```objective-c
// ios/MyCoreMLGeneratorModule.mm
#import <React/RCTBridgeModule.h>
#import <React/RCTEventEmitter.h>
#import "AIImageForge-Bridging-Header.h"

@interface RCT_EXTERN_MODULE(MyCoreMLGeneratorModule, RCTEventEmitter)

RCT_EXTERN_METHOD(loadModel:(NSString *)modelPath
                  resolve:(RCTPromiseResolveBlock)resolve
                  reject:(RCTPromiseRejectBlock)reject)

RCT_EXTERN_METHOD(generateImage:(NSString *)prompt
                  options:(NSDictionary *)options
                  resolve:(RCTPromiseResolveBlock)resolve
                  reject:(RCTPromiseRejectBlock)reject)

@end
```

2. Metal Fabric Component

Metal Shader

```metal
// ios/MyMetalImageView.metal
#include <metal_stdlib>
using namespace metal;

struct VertexOut {
    float4 position [[position]];
    float2 texCoord;
};

vertex VertexOut vertexShader(
    uint vertexID [[vertex_id]],
    constant float4x4 &matrix [[buffer(0)]],
    constant float2 *positions [[buffer(1)]],
    constant float2 *texCoords [[buffer(2)]]
) {
    VertexOut out;
    out.position = matrix * float4(positions[vertexID], 0.0, 1.0);
    out.texCoord = texCoords[vertexID];
    return out;
}

fragment float4 fragmentShader(
    VertexOut in [[stage_in]],
    texture2d<float> texture [[texture(0)]],
    constant float3 &tintColor [[buffer(0)]],
    constant float &tintIntensity [[buffer(1)]]
) {
    constexpr sampler textureSampler(mag_filter::linear, min_filter::linear);
    float4 color = texture.sample(textureSampler, in.texCoord);
    
    // Apply color tint with intensity control
    float3 tinted = mix(color.rgb, tintColor, tintIntensity);
    return float4(tinted, color.a);
}
```

Metal View Component

```objective-c
// ios/MyMetalImageView.mm
#import <React/RCTViewComponentView.h>
#import <MetalKit/MetalKit.h>
#import <simd/simd.h>

using namespace facebook::react;

@interface MyMetalImageView : RCTViewComponentView <MTKViewDelegate>
@end

@implementation MyMetalImageView {
    MTKView *_metalView;
    id<MTLDevice> _device;
    id<MTLCommandQueue> _commandQueue;
    id<MTLRenderPipelineState> _pipelineState;
    id<MTLTexture> _texture;
    simd_float4x4 _matrix;
    simd_float3 _tintColor;
    float _tintIntensity;
}

- (instancetype)initWithFrame:(CGRect)frame {
    if (self = [super initWithFrame:frame]) {
        _device = MTLCreateSystemDefaultDevice();
        _metalView = [[MTKView alloc] initWithFrame:self.bounds device:_device];
        _metalView.delegate = self;
        _metalView.clearColor = MTLClearColorMake(0.1, 0.1, 0.1, 1.0);
        _metalView.enableSetNeedsDisplay = YES;
        [self addSubview:_metalView];
        
        _commandQueue = [_device newCommandQueue];
        _tintColor = simd_make_float3(1.0, 0.5, 0.2); // Orange tint
        _tintIntensity = 0.3;
        
        [self setupPipeline];
    }
    return self;
}

- (void)setupPipeline {
    NSError *error = nil;
    id<MTLLibrary> library = [_device newDefaultLibrary];
    
    id<MTLFunction> vertexFunction = [library newFunctionWithName:@"vertexShader"];
    id<MTLFunction> fragmentFunction = [library newFunctionWithName:@"fragmentShader"];
    
    MTLRenderPipelineDescriptor *pipelineDescriptor = [MTLRenderPipelineDescriptor new];
    pipelineDescriptor.vertexFunction = vertexFunction;
    pipelineDescriptor.fragmentFunction = fragmentFunction;
    pipelineDescriptor.colorAttachments[0].pixelFormat = _metalView.colorPixelFormat;
    
    _pipelineState = [_device newRenderPipelineStateWithDescriptor:pipelineDescriptor error:&error];
    
    if (error) {
        NSLog(@"Failed to create pipeline state: %@", error);
    }
}

- (void)setImagePath:(NSString *)imagePath {
    if (!imagePath || imagePath.length == 0) return;
    
    UIImage *image = [UIImage imageWithContentsOfFile:imagePath];
    if (!image) return;
    
    // Create Metal texture from UIImage
    CGImageRef imageRef = image.CGImage;
    size_t width = CGImageGetWidth(imageRef);
    size_t height = CGImageGetHeight(imageRef);
    
    CGColorSpaceRef colorSpace = CGColorSpaceCreateDeviceRGB();
    void *imageData = calloc(width * height * 4, sizeof(uint8_t));
    
    CGContextRef context = CGBitmapContextCreate(
        imageData, width, height, 8, width * 4,
        colorSpace, kCGImageAlphaPremultipliedLast | kCGBitmapByteOrder32Big
    );
    
    CGContextDrawImage(context, CGRectMake(0, 0, width, height), imageRef);
    
    MTLTextureDescriptor *textureDescriptor = [MTLTextureDescriptor
        texture2DDescriptorWithPixelFormat:MTLPixelFormatRGBA8Unorm
        width:width
        height:height
        mipmapped:NO
    ];
    
    _texture = [_device newTextureWithDescriptor:textureDescriptor];
    
    MTLRegion region = {{0, 0, 0}, {width, height, 1}};
    [_texture replaceRegion:region
                mipmapLevel:0
                  withBytes:imageData
                bytesPerRow:width * 4];
    
    CGContextRelease(context);
    CGColorSpaceRelease(colorSpace);
    free(imageData);
    
    [_metalView setNeedsDisplay];
}

#pragma mark - MTKViewDelegate

- (void)drawInMTKView:(MTKView *)view {
    if (!_pipelineState || !_texture) return;
    
    id<MTLCommandBuffer> commandBuffer = [_commandQueue commandBuffer];
    MTLRenderPassDescriptor *renderPassDescriptor = view.currentRenderPassDescriptor;
    
    if (renderPassDescriptor) {
        id<MTLRenderCommandEncoder> renderEncoder = [commandBuffer
            renderCommandEncoderWithDescriptor:renderPassDescriptor
        ];
        
        [renderEncoder setRenderPipelineState:_pipelineState];
        
        // Set up vertex data
        float2 positions[4] = {
            {-1, -1}, {1, -1}, {-1, 1}, {1, 1}
        };
        float2 texCoords[4] = {
            {0, 1}, {1, 1}, {0, 0}, {1, 0}
        };
        
        simd_float4x4 matrix = matrix_identity_float4x4;
        
        [renderEncoder setVertexBytes:&matrix length:sizeof(matrix) atIndex:0];
        [renderEncoder setVertexBytes:positions length:sizeof(positions) atIndex:1];
        [renderEncoder setVertexBytes:texCoords length:sizeof(texCoords) atIndex:2];
        
        [renderEncoder setFragmentTexture:_texture atIndex:0];
        [renderEncoder setFragmentBytes:&_tintColor length:sizeof(_tintColor) atIndex:0];
        [renderEncoder setFragmentBytes:&_tintIntensity length:sizeof(_tintIntensity) atIndex:1];
        
        [renderEncoder drawPrimitives:MTLPrimitiveTypeTriangleStrip
                          vertexStart:0
                          vertexCount:4];
        
        [renderEncoder endEncoding];
        [commandBuffer presentDrawable:view.currentDrawable];
    }
    
    [commandBuffer commit];
}

- (void)mtkView:(MTKView *)view drawableSizeWillChange:(CGSize)size {
    // Handle resize if needed
}

@end
```

3. JavaScript Modules

CoreML Module Interface

```javascript
// js/modules/CoreMLGeneratorModule.js
import { NativeModules, NativeEventEmitter } from 'react-native';

const { MyCoreMLGeneratorModule } = NativeModules;

export const CoreMLGenerator = {
  loadModel: async (modelPath) => {
    return await MyCoreMLGeneratorModule.loadModel(modelPath);
  },
  
  generateImage: async (prompt, options = {}) => {
    return await MyCoreMLGeneratorModule.generateImage(prompt, options);
  },
};

export const generationEmitter = new NativeEventEmitter(
  NativeModules.MyCoreMLGeneratorModule || NativeModules.RCTEventEmitter
);
```

Metal Image Component

```javascript
// js/components/MetalImageView.js
import React from 'react';
import { requireNativeComponent, ViewPropTypes } from 'react-native';
import PropTypes from 'prop-types';

const NativeMetalImageView = requireNativeComponent('MyMetalImageView');

const MetalImageView = ({ imagePath, tintColor = [1, 0.5, 0.2], style }) => {
  return (
    <NativeMetalImageView
      style={style}
      imagePath={imagePath}
      tintColor={tintColor}
    />
  );
};

MetalImageView.propTypes = {
  imagePath: PropTypes.string,
  tintColor: PropTypes.arrayOf(PropTypes.number),
  style: ViewPropTypes.style,
};

export default MetalImageView;
```

4. Model Downloader Utility

```javascript
// js/utils/modelDownloader.js
import RNFS from 'react-native-fs';
import { unzip } from 'react-native-zip-archive';
import RNBlobUtil from 'react-native-blob-util';

const MODEL_URL = 'https://huggingface.co/apple/coreml-stable-diffusion-2-1-base/resolve/main/coreml-stable-diffusion-2-1-base_palettized.zip?download=true';
const MODEL_ID = 'stable-diffusion-2-1-base-palettized';

export const downloadModel = async (onProgress) => {
  const documentsDir = RNFS.DocumentDirectoryPath;
  const modelDir = `${documentsDir}/models/${MODEL_ID}`;
  const zipPath = `${documentsDir}/models/${MODEL_ID}.zip`;
  
  // Check if model already exists
  const modelExists = await RNFS.exists(`${modelDir}/CheckedModels`);
  if (modelExists) {
    console.log('Model already downloaded');
    return modelDir;
  }
  
  // Create directories if they don't exist
  await RNFS.mkdir(`${documentsDir}/models`, { NSURLIsExcludedFromBackupKey: true });
  
  console.log('Downloading model...');
  
  try {
    const response = await RNBlobUtil.config({
      fileCache: true,
      path: zipPath,
    }).fetch('GET', MODEL_URL, {
      'Content-Type': 'application/zip',
    });
    
    // Unzip the model
    console.log('Unzipping model...');
    const unzippedPath = await unzip(response.path(), `${documentsDir}/models`);
    
    // Clean up zip file
    await RNFS.unlink(zipPath);
    
    console.log('Model downloaded successfully');
    return unzippedPath;
  } catch (error) {
    console.error('Model download failed:', error);
    throw error;
  }
};

export const getModelPath = async () => {
  const modelPath = `${RNFS.DocumentDirectoryPath}/models/${MODEL_ID}`;
  return modelPath;
};
```

5. Main App Component

```javascript
// js/App.js
import React, { useState, useEffect, useCallback } from 'react';
import {
  SafeAreaView,
  StyleSheet,
  View,
  Text,
  TextInput,
  TouchableOpacity,
  ScrollView,
  Platform,
  ActivityIndicator,
} from 'react-native';
import { CoreMLGenerator, generationEmitter } from './modules/CoreMLGeneratorModule';
import MetalImageView from './components/MetalImageView';
import { downloadModel, getModelPath } from './utils/modelDownloader';

const App = () => {
  const [prompt, setPrompt] = useState('A beautiful sunset over mountains');
  const [generatedImage, setGeneratedImage] = useState(null);
  const [isGenerating, setIsGenerating] = useState(false);
  const [progress, setProgress] = useState(0);
  const [modelReady, setModelReady] = useState(false);
  const [error, setError] = useState(null);
  const [tintColor, setTintColor] = useState([1, 0.5, 0.2]); // Orange tint

  useEffect(() => {
    if (Platform.OS !== 'ios') {
      setError('This app requires iOS with Metal support');
      return;
    }

    const initializeModel = async () => {
      try {
        setError(null);
        console.log('Initializing model...');
        
        // Download model if needed
        await downloadModel();
        
        // Get model path and load
        const modelPath = await getModelPath();
        await CoreMLGenerator.loadModel(modelPath);
        
        setModelReady(true);
        console.log('Model ready for generation');
      } catch (err) {
        console.error('Model initialization failed:', err);
        setError(`Failed to initialize model: ${err.message}`);
      }
    };

    initializeModel();

    // Set up generation progress listener
    const progressSubscription = generationEmitter.addListener(
      'onGenerationStep',
      (event) => {
        setProgress(event.step / event.totalSteps);
      }
    );

    return () => {
      progressSubscription.remove();
    };
  }, []);

  const handleGenerate = useCallback(async () => {
    if (!prompt.trim() || !modelReady || isGenerating) return;

    setIsGenerating(true);
    setError(null);
    setGeneratedImage(null);
    setProgress(0);

    try {
      const options = {
        stepCount: 25,
        seed: Math.floor(Math.random() * 4294967295),
        guidanceScale: 7.5,
      };

      const imagePath = await CoreMLGenerator.generateImage(prompt, options);
      setGeneratedImage(`file://${imagePath}`);
    } catch (err) {
      console.error('Generation failed:', err);
      setError(`Generation failed: ${err.message}`);
    } finally {
      setIsGenerating(false);
    }
  }, [prompt, modelReady, isGenerating]);

  const handleTintChange = (colorIndex, value) => {
    const newTint = [...tintColor];
    newTint[colorIndex] = parseFloat(value);
    setTintColor(newTint);
  };

  return (
    <SafeAreaView style={styles.container}>
      <ScrollView contentContainerStyle={styles.scrollContent}>
        <Text style={styles.title}>AI Image Forge</Text>
        <Text style={styles.subtitle}>On-Device Image Generation</Text>

        {!modelReady && !error && (
          <View style={styles.loadingContainer}>
            <ActivityIndicator size="large" color="#007AFF" />
            <Text style={styles.loadingText}>Loading AI model...</Text>
          </View>
        )}

        {error && (
          <View style={styles.errorContainer}>
            <Text style={styles.errorText}>{error}</Text>
          </View>
        )}

        {modelReady && (
          <>
            <TextInput
              style={styles.input}
              placeholder="Describe the image you want to generate"
              placeholderTextColor="#999"
              value={prompt}
              onChangeText={setPrompt}
              multiline
              numberOfLines={3}
              editable={!isGenerating}
            />

            <TouchableOpacity
              style={[
                styles.generateButton,
                (!prompt.trim() || isGenerating) && styles.generateButtonDisabled,
              ]}
              onPress={handleGenerate}
              disabled={!prompt.trim() || isGenerating}
            >
              {isGenerating ? (
                <ActivityIndicator color="white" />
              ) : (
                <Text style={styles.generateButtonText}>Generate Image</Text>
              )}
            </TouchableOpacity>

            {isGenerating && (
              <View style={styles.progressContainer}>
                <Text style={styles.progressText}>
                  Generating... {Math.round(progress * 100)}%
                </Text>
                <View style={styles.progressBar}>
                  <View
                    style={[
                      styles.progressFill,
                      { width: `${progress * 100}%` },
                    ]}
                  />
                </View>
              </View>
            )}

            {generatedImage && (
              <View style={styles.resultContainer}>
                <Text style={styles.resultTitle}>Generated Image</Text>
                
                <View style={styles.shaderControls}>
                  <Text style={styles.controlLabel}>Tint Color:</Text>
                  {['Red', 'Green', 'Blue'].map((label, index) => (
                    <View key={label} style={styles.colorControl}>
                      <Text style={styles.colorLabel}>{label}</Text>
                      <TextInput
                        style={styles.colorInput}
                        value={tintColor[index].toString()}
                        onChangeText={(value) => handleTintChange(index, value)}
                        keyboardType="numeric"
                      />
                    </View>
                  ))}
                </View>

                <View style={styles.imageContainer}>
                  <MetalImageView
                    imagePath={generatedImage}
                    tintColor={tintColor}
                    style={styles.metalImage}
                  />
                </View>
              </View>
            )}
          </>
        )}
      </ScrollView>
    </SafeAreaView>
  );
};

const styles = StyleSheet.create({
  container: {
    flex: 1,
    backgroundColor: '#1a1a1a',
  },
  scrollContent: {
    padding: 20,
    paddingBottom: 40,
  },
  title: {
    fontSize: 32,
    fontWeight: 'bold',
    color: '#fff',
    textAlign: 'center',
    marginBottom: 8,
  },
  subtitle: {
    fontSize: 16,
    color: '#888',
    textAlign: 'center',
    marginBottom: 30,
  },
  loadingContainer: {
    alignItems: 'center',
    padding: 40,
  },
  loadingText: {
    color: '#fff',
    marginTop: 20,
    fontSize: 16,
  },
  errorContainer: {
    backgroundColor: '#ff3b30',
    padding: 15,
    borderRadius: 10,
    marginVertical: 20,
  },
  errorText: {
    color: '#fff',
    textAlign: 'center',
  },
  input: {
    backgroundColor: '#2c2c2e',
    borderRadius: 12,
    padding: 16,
    fontSize: 16,
    color: '#fff',
    minHeight: 100,
    textAlignVertical: 'top',
    marginBottom: 20,
  },
  generateButton: {
    backgroundColor: '#007AFF',
    borderRadius: 12,
    padding: 18,
    alignItems: 'center',
    marginBottom: 20,
  },
  generateButtonDisabled: {
    backgroundColor: '#555',
    opacity: 0.7,
  },
  generateButtonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  progressContainer: {
    marginBottom: 30,
  },
  progressText: {
    color: '#fff',
    fontSize: 14,
    marginBottom: 8,
    textAlign: 'center',
  },
  progressBar: {
    height: 4,
    backgroundColor: '#444',
    borderRadius: 2,
    overflow: 'hidden',
  },
  progressFill: {
    height: '100%',
    backgroundColor: '#007AFF',
  },
  resultContainer: {
    marginTop: 20,
  },
  resultTitle: {
    fontSize: 20,
    fontWeight: '600',
    color: '#fff',
    marginBottom: 20,
  },
  shaderControls: {
    backgroundColor: '#2c2c2e',
    borderRadius: 12,
    padding: 16,
    marginBottom: 20,
  },
  controlLabel: {
    color: '#fff',
    fontSize: 16,
    fontWeight: '500',
    marginBottom: 12,
  },
  colorControl: {
    flexDirection: 'row',
    alignItems: 'center',
    marginBottom: 8,
  },
  colorLabel: {
    color: '#fff',
    width: 60,
    fontSize: 14,
  },
  colorInput: {
    flex: 1,
    backgroundColor: '#3a3a3c',
    borderRadius: 8,
    padding: 8,
    color: '#fff',
    fontSize: 14,
  },
  imageContainer: {
    borderRadius: 16,
    overflow: 'hidden',
    backgroundColor: '#000',
  },
  metalImage: {
    width: '100%',
    aspectRatio: 1,
  },
});

export default App;
```

ðŸ”§ Codegen Configuration

Add to your package.json:

```json
{
  "codegenConfig": {
    "name": "AIImageForge",
    "type": "components",
    "jsSrcsDir": "./js",
    "android": {
      "javaPackageName": "com.aiimageforge"
    }
  }
}
```

Run codegen:

```bash
cd ios && RCT_NEW_ARCH_ENABLED=1 bundle exec pod install
```

ðŸ“Š Performance Optimization Tips

1. Model Selection: Use palettized (6-bit) models for iOS 17+
2. Compute Units: Configure cpuAndNeuralEngine for ANE acceleration
3. Memory Management: Enable reduceMemory mode
4. Image Resolution: Use 512x512 or 768x768 for balance of quality/speed
5. Step Count: 20-30 steps provides good quality with reasonable speed

ðŸ§ª Testing & Debugging

Xcode Instruments

1. Time Profiler: Identify CPU bottlenecks
2. Metal System Trace: Analyze GPU performance
3. Neural Engine: Monitor ANE utilization
4. Memory Debugger: Track memory usage

React Native Debugging

```bash
# Enable New Architecture debugging
npx react-native run-ios --simulator="iPhone 16" --variant=debug
```

ðŸš« Platform Limitations

Â· Android: CoreML/Metal are iOS-only. For Android, consider:
  Â· TensorFlow Lite with GPU delegate
  Â· ML Kit Image Generation
  Â· Fallback to server-based generation

ðŸ“¦ Deployment Considerations

1. App Store: Include model in app bundle or implement secure download
2. Privacy: On-device processing ensures data privacy
3. Size: Compressed models (~2GB) require careful storage management
4. Minimum iOS: 17.0+ for full palettized model support

ðŸ”„ Future Enhancements

1. Multiple Models: Support for different Stable Diffusion versions
2. Advanced Shaders: More complex Metal effects (bloom, stylization)
3. Batch Processing: Generate multiple images simultaneously
4. ControlNet: Add pose/edge guidance
5. Video Generation: Extend to short video clips

ðŸ†˜ Troubleshooting

Common Issues:

1. Model fails to load
   Â· Verify model files are in correct directory
   Â· Check file permissions
   Â· Ensure model is palettized for iOS 17+
2. Generation too slow
   Â· Reduce step count to 15-20
   Â· Use smaller resolution
   Â· Ensure ANE is enabled in compute units
3. Memory crashes
   Â· Enable reduceMemory mode
   Â· Close other memory-intensive apps
   Â· Add memory entitlement
4. Metal shader not working
   Â· Check Metal shader compilation logs
   Â· Verify texture format compatibility
   Â· Ensure proper view initialization

ðŸ“š Additional Resources

Â· Apple Stable Diffusion
Â· React Native New Architecture
Â· Metal Shading Language Guide
Â· CoreML Optimization Guide

This implementation provides a solid foundation for building sophisticated on-device AI applications with React Native's New Architecture. The combination of TurboModules for native logic and Fabric for UI components ensures optimal performance while maintaining the developer experience of React Native.